# **AI Agent Build Guide â€” Basic â†’ Kickâ€‘Ass**

  

A stepâ€‘byâ€‘step playbook for building [[AI agents]] in four tiers of increasing power. Each tier lists **goals, steps, deliverables, checklists, and guardrails**. Progress when the Success Criteria are met.

---

## **Table of Contents**

- [Tier 0 Â· Prereqs & Principles](#tier-0--prereqs--principles)
    
- [Tier 1 Â· Basic Agent (MVP Chat + Single Tool)](#tier-1--basic-agent-mvp-chat--single-tool)
    
- [Tier 2 Â· Intermediate Agent (RAG + Tools + Simple Memory)](#tier-2--intermediate-agent-rag--tools--simple-memory)
    
- [Tier 3 Â· Advanced Agent (Multiâ€‘Agent + Planning + Observability)](#tier-3--advanced-agent-multiagent--planning--observability)
    
- [Tier 4 Â· Kickâ€‘Ass Agent (Enterpriseâ€‘Grade, Selfâ€‘Improving)](#tier-4--kickass-agent-enterprisegrade-selfimproving)
    
- [Appendix A Â· Prompt & Schema Templates](#appendix-a--prompt--schema-templates)
    
- [Appendix B Â· Evaluation & Metrics](#appendix-b--evaluation--metrics)
    
- [Appendix C Â· Security & Compliance Checklist](#appendix-c--security--compliance-checklist)
    
- [Appendix D Â· Recommended Stack by Tier](#appendix-d--recommended-stack-by-tier)
    

---

## **Tier 0 Â· Prereqs & Principles**

  

**Mindset:** Build small, measure everything, ship often.

  

**Core Principles**

1. **Role & Outcome First** â€” Define who the agent serves and what artifact it must produce (text/JSON/report/action).
    
2. **Structured I/O** â€” Treat the agent like an API. Inputs/outputs are schemas, not vibes.

3. **Safety by Design** â€” Ethical rules, refusal cases, and redâ€‘team prompts from day one.
    
4. **Observability** â€” Logs, traces, and metrics or it didn't happen.
    

  

**Env Setup (minimum)**

- Version control (Git), Python â‰¥3.10, package manager (uv/pip/poetry), .env secrets, Docker optional.
    

  

**Milestone Gate**

- You can run a "hello world" LLM call locally and return JSON.
    

---

## **Tier 1 Â· Basic Agent (MVP Chat + Single Tool)**

  

**Goal:** A reliable singleâ€‘agent that answers in a narrow domain and can call **one tool** (e.g., calculator, web search, or DB lookup).

### **Steps**

1. **Define Role & Goal**
    
    - Example: _Expense Assistant_: "Given a receipt text, output a clean JSON expense record."
        
    
2. **Design Structured I/O**
    
    - Draft a Pydantic/JSON schema for input and output. Keep it tiny (5â€“10 fields max).
        
    
3. **Author the System Prompt**
    
    - Include role, tone, constraints, and a compact checklist (Do/Don't).
        
    
4. **Implement the Tool**
    
    - One tool with a typed function signature. Validate arguments.
        
    
5. **Ground Truth Samples**
    
    - Create 10â€“20 inputâ†’output pairs. Store as unit tests.
        
    
6. **Run Loop**
    
    - Call LLM â†’ parse JSON â†’ validate â†’ retry once on schema error.
        
    
7. **UI Stub**
    
    - CLI or simple FastAPI route (POST /run).
        
    
8. **Logging**
    
    - Capture inputs, outputs, latency, token usage, and tool calls.
        
    


### **Deliverables**

- system_prompt.md, schemas.py, tools.py, runner.py, tests, simple README.

### **Checklist**

- Outputs always validate against schema
    
- Max tokens & timeouts enforced
    
- Oneâ€‘shot retry on invalid JSON
    
- 90%+ pass on 20 sample tests
    
### **Success Criteria**

- Deterministic on fixed seed; produces valid JSON â‰¥ 95% of runs.
    

---

## **Tier 2 Â· Intermediate Agent (RAG + Tools + Simple Memory)**

**Goal:** Add **Retrievalâ€‘Augmented Generation**, multiple tools, and **episodic memory** for context over time.

### **Steps**

1. **Expand Role & Knowledge Boundary**
    
    - Clarify: what's in the knowledge base vs what must be refused.
        
    
2. **Document Ingest Pipeline**
    
    - Chunk â†’ embed â†’ store (Chroma/Milvus/FAISS). Add metadata (source, date, tags).
        
    
3. **RAG Query Flow**
    
    - Guardrail: query rewriting â†’ retrieve topâ€‘k â†’ context window budget.
        
    
4. **Add 2â€“3 Tools**
    
    - E.g., web fetch, calculator, DB read. Gate with whitelist.
        
    
5. **Episodic Memory**
    
    - Store conversation summaries + key facts (user prefs) with TTL.
        
    
6. **Policy/Refusal Layer**
    
    - Safety rules, compliance phrases, and escalation paths.
        
    
7. **Thin UI**
    
    - Minimal web UI (Gradio/Streamlit) or chat widget.
        
    
8. **Cache & Performance**
    
    - Add response cache; batch embeddings; measure latency.
        
    
9. **Eval Harness**
    
    - Create task sets: noâ€‘context, RAGâ€‘needed, refusalâ€‘needed. Track accuracy.
        
### **Deliverables**

- Ingest script, vector DB, RAG router, memory store, policies, expanded tests, lightweight UI.
    
### **Checklist**

- Topâ€‘k retrieval hits contain groundâ€‘truth answer â‰¥ 80%
    
- RAG off switch for A/B testing
    
- Refusal coverage for outâ€‘ofâ€‘scope requests
    
- PII filter for logs
    
### **Success Criteria**

- Beats Tierâ€‘1 accuracy by â‰¥ 20% on RAGâ€‘dependent tasks; stable latency (<2.5s p95) on 50th percentile queries.
    

---

## **Tier 3 Â· Advanced Agent (Multiâ€‘Agent + Planning + Observability)**

**Goal:** Introduce **planner/worker** separation, **multiâ€‘agent roles**, and productionâ€‘grade **observability**.

### **Steps**

1. **Agent Topology**
    
    - Roles: _Planner_, _Researcher_, _Executor_, _Critic_. Define responsibilities & I/O contracts.
        
    
2. **Task Planning**
    
    - Decompose user goal â†’ subâ€‘tasks with dependencies; depthâ€‘limit + budget.
        
    
3. **Toolformer Layer**
    
    - Central tool registry with auth, rate limits, JSON schema per tool.
        
    
4. **Reflection Loop**
    
    - Add selfâ€‘critique + repair (e.g., "Reflexion"-style) with guardrails.
        
    
5. **Knowledge Graph / GraphRAG (optional)**
    
    - Build entity/relation graph; retrieve by neighborhood.
        
    
6. **Longâ€‘Term Memory**
    
    - Separate vector RAG (knowledge) from memory (episodes/profiles). Add recall policies.
        
    
7. **Observability**
    
    - Structured logs, traces (OpenTelemetry), dashboards (Prometheus/Grafana).
        
    
8. **Evaluation at Scale**
    
    - Nightly regression on synthetic & real tasks; drift alerts.
        
    
9. **Resilience**
    
    - Circuit breakers, retries with backoff, fallbacks across model providers.
        
    
10. **Packaging & Deploy**
    

  

- Docker, CI, staged envs (dev/stage/prod), IaC (Terraform optional).

### **Deliverables**

- Orchestrator service, agent registry, planning policy, tracing middleware, CI pipeline, dashboards, synthetic eval suite.
    
### **Checklist**

- SLA: p95 latency target set and met
    
- Budgeting: max step/tool limits enforced
    
- Traces link steps â†’ tools â†’ tokens
    
- Canary release process documented
    
### **Success Criteria**

- Completes multiâ€‘step tasks with â‰¤1 critical error per 100 runs; MTTR < 30 min via onâ€‘call playbook.
    

---

## **Tier 4 Â· Kickâ€‘Ass Agent (Enterpriseâ€‘Grade, Selfâ€‘Improving)**

**Goal:** A hardened, scalable, **selfâ€‘optimizing** agent platform with strong governance, safety, and economic efficiency.
### **Steps**

1. **Constitutional Layer**
    
    - Values, rules, refusal policies, priority resolution; redâ€‘team prompts.
        
    
2. **Policyâ€‘Driven Orchestration**
    
    - Route by task type, risk level, and cost/latency constraints.
        
    
3. **Model Mesh**
    
    - Multiâ€‘provider, multiâ€‘modal routing; autoscaling; GPU/offload strategies.
        
    
4. **Data Contracts & Governance**
    
    - Schemas versioned; lineage; retention; DLP; encryption at rest/in transit.
        
    
5. **Advanced Memory**
    
    - Hybrid memory: vector + graph + keyâ€‘value; TTLs; decay heuristics; privacy scopes.
        
    
6. **Active Learning & Autoâ€‘Eval**
    
    - Collect user feedback; autoâ€‘label; retrain retrieval index; prompt evolution.
        
    
7. **Reward Models / Preference Optimization (optional)**
    
    - Use ratings to fineâ€‘tune prompts or small adapters.
        
    
8. **Cost & Carbon Controls**
    
    - Token spend budget; dynamic routing to cheaper models; batch & cache.
        
    
9. **Incident Response & Abuse Handling**
    
    - Playbooks, audit trails, rollbacks, killâ€‘switch.
        
    
10. **Enterprise Integrations**
    

- SSO, SCIM, roleâ€‘based access, secret management, audit exports.
    
### **Deliverables**

- Constitution file(s), policy router, model gateway, governance docs, autoâ€‘eval pipeline, cost dashboards, incident run-books.

### **Checklist**

- Monthly redâ€‘team runs; findings tracked
    
- Cost per task & per user monitored
    
- Privacy impact assessment signed off
    
- Disaster recovery test performed
    
### **Success Criteria**

- Safe, fast, lowâ€‘cost operations at scale with measurable monthâ€‘overâ€‘month improvement in quality and unit economics.
    

---

## **Appendix A Â· Prompt & Schema Templates**

  

### **A1. Compact System Prompt (fillâ€‘in)**

```
You are <ROLE>, serving <AUDIENCE>. Your job: <OUTCOME>.
Follow the rules:
1) Output must match schema exactly.
2) Use tools only when needed.
3) Refuse if request is unsafe/outâ€‘ofâ€‘scope; suggest alternatives.
4) Think stepâ€‘byâ€‘step but return only the final JSON.
```

### **A2. Output JSON Schema (example)**

```
{
  "title": "AgentOutput",
  "type": "object",
  "properties": {
    "status": {"type": "string", "enum": ["ok", "error"]},
    "answer": {"type": "string"},
    "citations": {"type": "array", "items": {"type": "string"}},
    "cost_tokens": {"type": "integer", "minimum": 0}
  },
  "required": ["status", "answer"],
  "additionalProperties": false
}
```

### **A3. Tool Signature (example)**

```
{
  "name": "search_web",
  "description": "Query the web and return top results",
  "parameters": {
    "type": "object",
    "properties": {
      "query": {"type": "string"},
      "k": {"type": "integer", "minimum": 1, "maximum": 10}
    },
    "required": ["query"],
    "additionalProperties": false
  }
}
```

---

## **Appendix B Â· Evaluation & Metrics**

**Functional**: exactâ€‘match, F1/ROUGE, task success rate, hallucination rate.

**UX**: CSAT, deflection rate, timeâ€‘toâ€‘answer.

**Ops**: p50/p95 latency, error rate, token spend per task, cache hit rate.

**Safety**: jailbreak success rate, refusal correctness, PII leakage.


**Test Sets**

- Golden set (handâ€‘labeled), synthetic variations, adversarial prompts, regression suite.
    

**Gates**

- Promote a model/prompt only if it improves â‰¥ X% on target metrics and doesn't regress safety.
    

---

## **Appendix C Â· Security & Compliance Checklist**

- Secrets in vault; no secrets in logs
    
- PII masking/hashed IDs; data minimization
    
- Encryption in transit (TLS) and at rest
    
- Access control: RBAC, least privilege, audit logs
    
- Data retention policy with TTLs
    
- Vendor & model risk review
    

---

## **Appendix D Â· Recommended Stack by Tier**

**Tier 1**: FastAPI, Pydantic, OpenAI/Claude API, one tool, pytest, simple logs.

**Tier 2**: + Chroma/Milvus/FAISS, LangChain/LlamaIndex, small memory store, Gradio/Streamlit.

**Tier 3**: + Orchestrator (CrewAI/LangGraph/OpenAI Assistants + Tools), OpenTelemetry, Prometheus/Grafana, Docker, CI.

**Tier 4**: + Model gateway (router), policy engine (Constitution), multiâ€‘provider backends, feature store, autoâ€‘eval pipelines, SSO/RBAC, cost dashboards.

---

### **How to Use This Guide**

1. Pick your tier and copy the steps as a checklist.
    
2. Fill in the prompt/schema templates.
    
3. Build tests before you wire tools.
    
4. Add observability as soon as something works.
    
5. Move up a tier only when Success Criteria are met.



If you internalize the flow from **Tier 1 â†’ Tier 4**, you'll be able to architect practically any agentic system, from a lightweight local assistant to a self-improving multi-agent platform like your **AQLAI _Nexus**.

Here's a quick way to memorize it efficiently:

---

### **ğŸ§  Memory Hack Plan**

**1ï¸âƒ£ Chunk by Tier**

- **Tier 1 â€“ Core Mechanics:** Role, Schema, Prompt, One Tool, Test.
    
- **Tier 2 â€“ Context Engine:** RAG, Tools, Memory, Policy.
    
- **Tier 3 â€“ Orchestration:** Planner + Workers, Reflection, Observability.
    
- **Tier 4 â€“ Autonomy:** Constitution, Governance, Optimization, Scale.
    

**2ï¸âƒ£ Use Mnemonics**


> **"Real Smart Reasoners Make Awesome Code"**

> _Role â†’ Schema â†’ Reasoning â†’ Memory â†’ Automation â†’ Control._


**3ï¸âƒ£ Visualize**

Picture each tier as a **pyramid** building upward:

foundation (LLM + tool) â†’ walls (RAG + memory) â†’ floors (multi-agents + planning) â†’ crown (autonomy + self-improvement).

**4ï¸âƒ£ Apply Immediately**

Build a **Tier 1 mini-agent tonight**â€”even something trivial like a JSON summarizer. Each working prototype locks the memory deeper.

**5ï¸âƒ£ Teach It**

Explain the four tiers aloud (or record a 2-minute voice note). Teaching is retention gold.

---