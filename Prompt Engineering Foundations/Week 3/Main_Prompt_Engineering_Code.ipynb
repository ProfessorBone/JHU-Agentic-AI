{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyS2Ovli5gq2"
   },
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "In this notebook, we will demonstrate the fundementals of using LangChain for prompt engineering. Specifically, we will do the following:\n",
    "\n",
    "- create a prompt from a template\n",
    "- create a LLM\n",
    "- create a chain\n",
    "- look at some specialized chains for few-shot prompting\n",
    "\n",
    "For this  exercise, we are going to focus on a classification task. Namely, the classification of the \"stance\" of a comment towards another comment. The base comment is given below:\n",
    "\n",
    "```python\n",
    "comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
    "```\n",
    "\n",
    "The replies to the comment that we will classify for their stance toward the comment as \"agree\", \"disagree\", and \"neutral\" are:\n",
    "\n",
    "```python\n",
    "replies = [\n",
    "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
    "    \"Frank Herbert wrote a lot of books.\",\n",
    "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
    "    \"The quick red fox jumped over the lazy brown dog.\",\n",
    "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--I4xWYW5gq7"
   },
   "source": [
    "## Configure the environment\n",
    "\n",
    "I need your help with classifying the stance of replies to comments about a topic using LangChain and Langchain-Huggingface for running local models. First, I need the code to install the neccesary packages from a notebook envrionment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RowDP8ff5gq8",
    "outputId": "5acd83cf-93c8-47a9-e414-bb9d19245bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (0.4.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: transformers in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-huggingface in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.34.4)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-huggingface) (0.34.4)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torch in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Requirement already satisfied: accelerate in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/faheem/Documents/Greater Learning/Agentic AI/agentic_ai_env/lib/python3.13/site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages for LangChain stance classification with local models\n",
    "\n",
    "%pip install langchain\n",
    "%pip install transformers\n",
    "%pip install langchain-huggingface\n",
    "%pip install torch\n",
    "%pip install accelerate\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a prompt object\n",
    "I need your help with classifying the stance of comments using LangChain. First, I need you to give me the code to create a prompt object, called \"stance_prompt\" from LangChain around the following template: '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label. comment: {comment} reply: {reply} stance:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NU4fDbQB5gq_",
    "outputId": "dac58bc9-1e60-4d91-9866-ca7c1e850eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
      "comment: I think this policy is not effective.\n",
      "reply: I agree, it doesn't address the core issues.\n",
      "stance:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the template for stance classification\n",
    "template = '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the prompt object\n",
    "stance_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Example usage - Test the prompt formatting\n",
    "comment = \"I think this policy is not effective.\"\n",
    "reply = \"I agree, it doesn't address the core issues.\"\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_prompt = stance_prompt.format(comment=comment, reply=reply)\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSSsDeKQ5gq_"
   },
   "source": [
    "## Create an LLM object\n",
    "\n",
    "### Option 1: Using a small encoder-decoder model\n",
    "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text2text-generation model of \"declare-lab/flan-alpaca-gpt4-xl\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not text from the prompt.\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model using Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"declare-lab/flan-alpaca-gpt4-xl\",\n",
    "    device=0,  # Use GPU (-1 for CPU)\n",
    "    max_new_tokens = 500,\n",
    ")\n",
    "\n",
    "# Create the LangChain LLM using the HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Example usage with the prompt object from before\n",
    "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
    "comment: I think the new policy will help improve efficiency.\n",
    "reply: I disagree, the policy doesn't address the real issues.\n",
    "stance:'''\n",
    "\n",
    "# Get the model's response\n",
    "response = llm(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### Option 2: Using a small decoder-only model\n",
    "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text-generation model of \"tiiuae/Falcon3-1B-Instruct\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not the prompt.\n",
    "\n",
    "```python\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/Falcon3-3B-Instruct\",\n",
    "    device=0,  # Use GPU (-1 for CPU)\n",
    "    max_new_tokens = 500,\n",
    "    return_full_text=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "57503e437a0947068985d92302bb5a99",
      "090f0fb4b5b043df91280f4a05f2a110",
      "4dd918f565a446fa9b8e77b87a9a56fa",
      "f6ed95119f5242ae900c56c0b14dafbc",
      "af778a7c76aa4f8ea1c190394667d8e9",
      "9db64bbd996c4b1eb55346c051848225",
      "ac1dec2fcbd848578e9c87ca57040b30",
      "ed0ff2bb755f41ef9070e8ee0af0b7b3",
      "832e195a263048909254c5ccd87e8c62",
      "2079f966cb1348f29ce87ab007e79eaa"
     ]
    },
    "id": "GcJxbbQG5gq_",
    "outputId": "9d6c565e-a370-400f-a67b-aedb54172d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "MPS (Apple Silicon GPU): True\n",
      "CPU cores: Available\n",
      "Using device: MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a6fce8d3c64d09bfe79ac775195168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the LLM...\n",
      "Input prompt:\n",
      "Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
      "comment: I think the new policy will help improve efficiency.\n",
      "reply: I disagree, the policy doesn't address the real issues.\n",
      "stance:\n",
      "\n",
      "Model response:\n",
      " disagree\n",
      " disagree\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check available devices on your Mac\n",
    "print(\"Available devices:\")\n",
    "print(f\"MPS (Apple Silicon GPU): {torch.backends.mps.is_available()}\")\n",
    "print(f\"CPU cores: Available\")\n",
    "\n",
    "# Use MPS if available, fallback to CPU\n",
    "device = 0 if torch.backends.mps.is_available() else -1\n",
    "print(f\"Using device: {'MPS (Apple Silicon GPU)' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Create the Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/Falcon3-3B-Instruct\",\n",
    "    device=device,  # Will use MPS (Apple Silicon GPU) if available\n",
    "    max_new_tokens=500,\n",
    "    return_full_text=False  # Only return generated text, not the prompt\n",
    ")\n",
    "\n",
    "# Create the LangChain LLM using the HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Test the LLM with a simple example\n",
    "test_prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
    "comment: I think the new policy will help improve efficiency.\n",
    "reply: I disagree, the policy doesn't address the real issues.\n",
    "stance:'''\n",
    "\n",
    "# Get the model's response\n",
    "print(\"Testing the LLM...\")\n",
    "print(\"Input prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\nModel response:\")\n",
    "response = llm.invoke(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h0G4UPb5grA"
   },
   "source": [
    "## Create a Chain\n",
    "\n",
    "Now, I would like the python code to create a LangChain Chain from the prompt template \"stance_prompt\" and the LLM \"llm\". Make sure to use the \"|\" syntax for defining the chain. and call the chain by the \"invoke\" method. Please name the chain \"stance_chain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Basic stance prompt created successfully!\n",
      "Template: Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputting the label.\n",
      "\n",
      "comment: {comment}\n",
      "reply: {reply}\n",
      "stance:\n"
     ]
    }
   ],
   "source": [
    "# Create the basic stance prompt template\n",
    "stance_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template='''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputting the label.\n",
    "\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    ")\n",
    "\n",
    "print(\" Basic stance prompt created successfully!\")\n",
    "print(\"Template:\", stance_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W18QHavF5grB",
    "outputId": "3e140af9-82f9-444c-f8ac-cbd99222c1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain created successfully!\n",
      "Chain components: first=PromptTemplate(input_variables=['comment', 'reply'], input_types={}, partial_variables={}, template='Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputting the label.\\n\\ncomment: {comment}\\nreply: {reply}\\nstance:') middle=[] last=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x12c148ec0>, model_id='tiiuae/Falcon3-3B-Instruct')\n",
      "\n",
      "Testing the complete chain...\n",
      "==================================================\n",
      "Comment: The new delivery route optimization will save us fuel costs.\n",
      "Reply: I disagree, the system is too complicated and won't really help.\n",
      "==================================================\n",
      "Chain result:\n",
      "Stance:  disagree\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the chain using the \"|\" (pipe) syntax\n",
    "stance_chain = stance_prompt | llm\n",
    "\n",
    "print(\"Chain created successfully!\")\n",
    "print(\"Chain components:\", stance_chain)\n",
    "print()\n",
    "\n",
    "# Example usage: Test the complete chain\n",
    "comment = \"The new delivery route optimization will save us fuel costs.\"\n",
    "reply = \"I disagree, the system is too complicated and won't really help.\"\n",
    "\n",
    "# Use invoke method with input as a dictionary\n",
    "print(\"Testing the complete chain...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Comment: {comment}\")\n",
    "print(f\"Reply: {reply}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Invoke the chain with the input dictionary\n",
    "result = stance_chain.invoke({\n",
    "    \"comment\": comment, \n",
    "    \"reply\": reply\n",
    "})\n",
    "\n",
    "print(\"Chain result:\")\n",
    "print(f\"Stance: {result}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgrinvs_5grC"
   },
   "source": [
    "Great. Now, I would like to code to run the previously defined \"stance_chain\" on a comment called \"test_comment\" across each entry in a list called \"test_replies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x9tSvtUB5grC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance classification:\n",
      "======================================================================\n",
      "Original Comment: The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\n",
      "======================================================================\n",
      "\n",
      "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stance_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreply\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Use the stance_chain to get classification\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[43mstance_chain\u001b[49m.invoke({\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m: test_comment,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreply\u001b[39m\u001b[33m\"\u001b[39m: reply\n\u001b[32m     28\u001b[39m })\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Store result and display\u001b[39;00m\n\u001b[32m     31\u001b[39m results.append(result.strip())  \u001b[38;5;66;03m# Remove any extra whitespace\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'stance_chain' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the test comment and multiple replies\n",
    "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
    "\n",
    "test_replies = [\n",
    "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
    "    \"Frank Herbert wrote a lot of books.\",\n",
    "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
    "    \"The quick red fox jumped over the lazy brown dog.\",\n",
    "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
    "]\n",
    "\n",
    "print(\"Testing stance classification:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original Comment: {test_comment}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Run stance classification for each reply\n",
    "results = []\n",
    "\n",
    "for i, reply in enumerate(test_replies, 1):\n",
    "    print(f\"Reply {i}: {reply}\")\n",
    "    \n",
    "    # Use the stance_chain to get classification\n",
    "    result = stance_chain.invoke({\n",
    "        \"comment\": test_comment,\n",
    "        \"reply\": reply\n",
    "    })\n",
    "    \n",
    "    # Store result and display\n",
    "    results.append(result.strip())  # Remove any extra whitespace\n",
    "    print(f\"Stance: {result.strip()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY OF RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (reply, stance) in enumerate(zip(test_replies, results), 1):\n",
    "    # Truncate long replies for summary\n",
    "    short_reply = reply[:40] + \"...\" if len(reply) > 40 else reply\n",
    "    print(f\"{i}. [{stance.upper()}] {short_reply}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljgw93Vr5grD"
   },
   "source": [
    "# Few-shot Prompt\n",
    "\n",
    "Now, Please create a LangChain FewShotPromptTemplate for classifying the stance of a reply to a comment. Use the following template for each example and create an example prompt using example_prompt for the examples in the few-shot prompt template:\n",
    "\n",
    "comment: [comment]\n",
    "reply: [reply]\n",
    "stance: [stance]\n",
    "\n",
    "Then, use the following structure for the few-shot prompt:\n",
    "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
    "\n",
    "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "Create five few-shot examples with different comments and replies, including at least one for each possible stance: \"agree\", \"disagree\", and \"neutral\". Provide the code that constructs the FewShotPromptTemplate using the examples and the given prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mLuNW7HJ5grD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Prompt Template Created!\n",
      "======================================================================\n",
      "Formatted Few-Shot Prompt:\n",
      "======================================================================\n",
      "Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.\n",
      "\n",
      "comment: I think the new policy will help improve efficiency.\n",
      "reply: I agree, it will definitely streamline our operations.\n",
      "stance: agree\n",
      "\n",
      "comment: The new education reform seems promising.\n",
      "reply: I disagree, it doesn't address the underlying issues.\n",
      "stance: disagree\n",
      "\n",
      "comment: The park renovation project is a good idea.\n",
      "reply: I'm not sure. The location might be an issue.\n",
      "stance: neutral\n",
      "\n",
      "comment: Artificial intelligence will revolutionize healthcare.\n",
      "reply: Absolutely, it has the potential to save many lives.\n",
      "stance: agree\n",
      "\n",
      "comment: The economy is showing signs of recovery after the pandemic.\n",
      "reply: I disagree, the recovery seems slow and uneven across sectors.\n",
      "stance: disagree\n",
      "\n",
      "Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
      "comment: The new delivery route optimization will save us fuel costs.\n",
      "reply: I'm not convinced it will make much difference.\n",
      "stance:\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Define the template for each individual example\n",
    "example_template = '''comment: {comment}\n",
    "reply: {reply}\n",
    "stance: {stance}'''\n",
    "\n",
    "# Create the example prompt template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Create five diverse few-shot examples covering all stances\n",
    "examples = [\n",
    "    {\n",
    "        'comment': \"I think the new policy will help improve efficiency.\",\n",
    "        'reply': \"I agree, it will definitely streamline our operations.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The new education reform seems promising.\", \n",
    "        'reply': \"I disagree, it doesn't address the underlying issues.\",\n",
    "        'stance': 'disagree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The park renovation project is a good idea.\",\n",
    "        'reply': \"I'm not sure. The location might be an issue.\",\n",
    "        'stance': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Artificial intelligence will revolutionize healthcare.\",\n",
    "        'reply': \"Absolutely, it has the potential to save many lives.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The economy is showing signs of recovery after the pandemic.\",\n",
    "        'reply': \"I disagree, the recovery seems slow and uneven across sectors.\",\n",
    "        'stance': 'disagree'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the prefix and suffix as provided\n",
    "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
    "\n",
    "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the FewShotPromptTemplate\n",
    "few_shot_stance_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Test the few-shot prompt\n",
    "print(\"Few-Shot Prompt Template Created!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example usage - format the template with new data\n",
    "test_comment = \"The new delivery route optimization will save us fuel costs.\"\n",
    "test_reply = \"I'm not convinced it will make much difference.\"\n",
    "\n",
    "formatted_few_shot_prompt = few_shot_stance_prompt.format(\n",
    "    comment=test_comment,\n",
    "    reply=test_reply\n",
    ")\n",
    "\n",
    "print(\"Formatted Few-Shot Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(formatted_few_shot_prompt)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Prompt Template Created!\n",
      "======================================================================\n",
      "Few-Shot Chain Created!\n",
      "Testing few-shot vs regular prompt...\n",
      "======================================================================\n",
      "COMPARISON TEST:\n",
      "Comment: The new delivery route optimization will save us fuel costs.\n",
      "Reply: I'm not convinced it will make much difference.\n",
      "\n",
      "Regular prompt result:\n",
      "Stance: neutral\n",
      "\n",
      "Few-shot prompt result:\n",
      "Stance: neutral\n",
      "======================================================================\n",
      "Formatted Few-Shot Prompt:\n",
      "======================================================================\n",
      "Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.\n",
      "\n",
      "comment: I think the new policy will help improve efficiency.\n",
      "reply: I agree, it will definitely streamline our operations.\n",
      "stance: agree\n",
      "\n",
      "comment: The new education reform seems promising.\n",
      "reply: I disagree, it doesn't address the underlying issues.\n",
      "stance: disagree\n",
      "\n",
      "comment: The park renovation project is a good idea.\n",
      "reply: I'm not sure. The location might be an issue.\n",
      "stance: neutral\n",
      "\n",
      "comment: Artificial intelligence will revolutionize healthcare.\n",
      "reply: Absolutely, it has the potential to save many lives.\n",
      "stance: agree\n",
      "\n",
      "comment: The economy is showing signs of recovery after the pandemic.\n",
      "reply: I disagree, the recovery seems slow and uneven across sectors.\n",
      "stance: disagree\n",
      "\n",
      "Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
      "comment: The new delivery route optimization will save us fuel costs.\n",
      "reply: I'm not convinced it will make much difference.\n",
      "stance:\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Define the template for each individual example\n",
    "example_template = '''comment: {comment}\n",
    "reply: {reply}\n",
    "stance: {stance}'''\n",
    "\n",
    "# Create the example prompt template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Create five diverse few-shot examples covering all stances\n",
    "examples = [\n",
    "    {\n",
    "        'comment': \"I think the new policy will help improve efficiency.\",\n",
    "        'reply': \"I agree, it will definitely streamline our operations.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The new education reform seems promising.\", \n",
    "        'reply': \"I disagree, it doesn't address the underlying issues.\",\n",
    "        'stance': 'disagree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The park renovation project is a good idea.\",\n",
    "        'reply': \"I'm not sure. The location might be an issue.\",\n",
    "        'stance': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Artificial intelligence will revolutionize healthcare.\",\n",
    "        'reply': \"Absolutely, it has the potential to save many lives.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The economy is showing signs of recovery after the pandemic.\",\n",
    "        'reply': \"I disagree, the recovery seems slow and uneven across sectors.\",\n",
    "        'stance': 'disagree'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the prefix and suffix as provided\n",
    "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
    "\n",
    "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the FewShotPromptTemplate\n",
    "few_shot_stance_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Test the few-shot prompt\n",
    "print(\"Few-Shot Prompt Template Created!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define a mock LLM for demonstration (replace with a real LLM as needed)\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "# This mock LLM will always return \"neutral\" for demonstration\n",
    "llm = FakeListLLM(responses=[\"neutral\"])\n",
    "\n",
    "# Create a new chain using the few-shot prompt template\n",
    "few_shot_stance_chain = few_shot_stance_prompt | llm\n",
    "\n",
    "print(\"Few-Shot Chain Created!\")\n",
    "print(\"Testing few-shot vs regular prompt...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compare regular vs few-shot on the same example\n",
    "test_comment = \"The new delivery route optimization will save us fuel costs.\"\n",
    "test_reply = \"I'm not convinced it will make much difference.\"\n",
    "\n",
    "print(\"COMPARISON TEST:\")\n",
    "print(f\"Comment: {test_comment}\")\n",
    "print(f\"Reply: {test_reply}\")\n",
    "print()\n",
    "\n",
    "# Create a regular prompt template for comparison\n",
    "regular_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template='''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    ")\n",
    "\n",
    "# Create a regular stance_chain using the regular prompt and llm\n",
    "regular_stance_chain = regular_prompt | llm\n",
    "\n",
    "# Test regular stance_chain\n",
    "print(\"Regular prompt result:\")\n",
    "regular_result = regular_stance_chain.invoke({\n",
    "    \"comment\": test_comment,\n",
    "    \"reply\": test_reply\n",
    "})\n",
    "print(f\"Stance: {regular_result.strip()}\")\n",
    "print()\n",
    "\n",
    "# Test few-shot chain\n",
    "print(\"Few-shot prompt result:\")\n",
    "few_shot_result = few_shot_stance_chain.invoke({\n",
    "    \"comment\": test_comment,\n",
    "    \"reply\": test_reply  \n",
    "})\n",
    "print(f\"Stance: {few_shot_result.strip()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example usage - format the template with new data\n",
    "test_comment = \"The new delivery route optimization will save us fuel costs.\"\n",
    "test_reply = \"I'm not convinced it will make much difference.\"\n",
    "\n",
    "formatted_few_shot_prompt = few_shot_stance_prompt.format(\n",
    "    comment=test_comment,\n",
    "    reply=test_reply\n",
    ")\n",
    "\n",
    "print(\"Formatted Few-Shot Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(formatted_few_shot_prompt)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompt\n",
    "\n",
    "Please generate code that uses chain-of-thought (CoT) prompting to classify the stance of a reply to a comment. The process should consist of two stages:\n",
    "\n",
    "1. First Stage (Explanatory Step):\n",
    "- Generate an explanation for the stance (agree, disagree, neutral) of the reply toward the comment.\n",
    "- Use a prompt template for this step with the variables \"comment\" and \"reply\".\n",
    "- Output: \"stance_reason\".\n",
    "\n",
    "2. Second Stage (Final Classification Step):\n",
    "- Based on the explanation from the first stage (\"stance_reason\"), determine the final stance of the reply.\n",
    "- Use a second prompt template for this step with the variables \"comment\", \"reply\", and \"stance_reason\".\n",
    "- Output: The final stance as \"agree\", \"disagree\", or \"neutral\".\n",
    "\n",
    "Use the \"RunnablePassthrough\" to pass the \"comment\" and \"reply\" variables from the first chain to the second chain, and chain the steps together using the \"|\" operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Implementing Chain-of-Thought (CoT) Stance Classification\n",
      "======================================================================\n",
      " Stage 1 Prompt Template Created (Explanatory Step)\n",
      " Stage 2 Prompt Template Created (Final Classification)\n",
      " Chain-of-Thought Stance Classification Chain Created!\n",
      " Chain Structure: Input  Reasoning  Classification  Output\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Chain-of-Thought Prompt Implementation\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\" Implementing Chain-of-Thought (CoT) Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Stage 1: Explanatory Step - Generate reasoning for the stance\n",
    "reasoning_template = \"\"\"Analyze the relationship between the following comment and reply. \n",
    "Provide a detailed explanation of why the reply would be classified as \"agree\", \"disagree\", or \"neutral\" toward the comment.\n",
    "Focus on the key indicators in the reply that show the stance.\n",
    "\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "\n",
    "Explanation of stance reasoning:\"\"\"\n",
    "\n",
    "reasoning_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template=reasoning_template\n",
    ")\n",
    "\n",
    "print(\" Stage 1 Prompt Template Created (Explanatory Step)\")\n",
    "\n",
    "# Stage 2: Final Classification Step - Use reasoning to determine final stance\n",
    "classification_template = \"\"\"Based on the explanation provided, classify the stance of the reply toward the comment.\n",
    "\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "Reasoning: {stance_reason}\n",
    "\n",
    "Based on this reasoning, the stance of the reply toward the comment is (respond with only one word - \"agree\", \"disagree\", or \"neutral\"):\"\"\"\n",
    "\n",
    "classification_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"stance_reason\"],\n",
    "    template=classification_template\n",
    ")\n",
    "\n",
    "print(\" Stage 2 Prompt Template Created (Final Classification)\")\n",
    "\n",
    "# Create the Chain-of-Thought chain using RunnablePassthrough\n",
    "# This creates a two-stage pipeline where:\n",
    "# 1. First stage generates reasoning\n",
    "# 2. Second stage uses that reasoning to make final classification\n",
    "\n",
    "cot_stance_chain = (\n",
    "    {\n",
    "        \"comment\": RunnablePassthrough(),\n",
    "        \"reply\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        stance_reason=reasoning_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    | classification_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\" Chain-of-Thought Stance Classification Chain Created!\")\n",
    "print(\" Chain Structure: Input  Reasoning  Classification  Output\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing Chain-of-Thought Stance Classification\n",
      "======================================================================\n",
      " Test Case:\n",
      "Comment: The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\n",
      "Reply: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
      "\n",
      " Chain-of-Thought Processing...\n",
      "--------------------------------------------------\n",
      " Final Classification: agree\n",
      "======================================================================\n",
      "\n",
      " Comparison: CoT vs Regular Classification\n",
      "======================================================================\n",
      "\n",
      " Test Case 1:\n",
      "Comment: The new policy will improve workplace efficiency.\n",
      "Reply: I agree completely, it addresses our main concerns.\n",
      " Final Classification: agree\n",
      "======================================================================\n",
      "\n",
      " Comparison: CoT vs Regular Classification\n",
      "======================================================================\n",
      "\n",
      " Test Case 1:\n",
      "Comment: The new policy will improve workplace efficiency.\n",
      "Reply: I agree completely, it addresses our main concerns.\n",
      " CoT Result: agree\n",
      " Regular Result: agree\n",
      "<|assistant|>\n",
      "agree\n",
      " Expected: agree\n",
      "----------------------------------------\n",
      "\n",
      " Test Case 2:\n",
      "Comment: Remote work is the future of employment.\n",
      "Reply: I disagree, in-person collaboration is still essential.\n",
      " CoT Result: agree\n",
      " Regular Result: agree\n",
      "<|assistant|>\n",
      "agree\n",
      " Expected: agree\n",
      "----------------------------------------\n",
      "\n",
      " Test Case 2:\n",
      "Comment: Remote work is the future of employment.\n",
      "Reply: I disagree, in-person collaboration is still essential.\n",
      " CoT Result: <|assistant|>\n",
      "disagree\n",
      " Regular Result: disagree\n",
      " Expected: disagree\n",
      "----------------------------------------\n",
      "\n",
      " Test Case 3:\n",
      "Comment: The weather has been quite unpredictable this month.\n",
      "Reply: That's an interesting observation about climate patterns.\n",
      " CoT Result: <|assistant|>\n",
      "disagree\n",
      " Regular Result: disagree\n",
      " Expected: disagree\n",
      "----------------------------------------\n",
      "\n",
      " Test Case 3:\n",
      "Comment: The weather has been quite unpredictable this month.\n",
      "Reply: That's an interesting observation about climate patterns.\n",
      " CoT Result: <|assistant|>\n",
      "neutral\n",
      " Regular Result: neutral\n",
      " Expected: neutral\n",
      "----------------------------------------\n",
      "\n",
      " Chain-of-Thought testing completed!\n",
      " CoT Result: <|assistant|>\n",
      "neutral\n",
      " Regular Result: neutral\n",
      " Expected: neutral\n",
      "----------------------------------------\n",
      "\n",
      " Chain-of-Thought testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the Chain-of-Thought Stance Classification\n",
    "print(\" Testing Chain-of-Thought Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with the original Dune movie example\n",
    "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
    "test_reply = \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\"\n",
    "\n",
    "print(\" Test Case:\")\n",
    "print(f\"Comment: {test_comment}\")\n",
    "print(f\"Reply: {test_reply}\")\n",
    "print()\n",
    "print(\" Chain-of-Thought Processing...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Invoke the CoT chain\n",
    "result = cot_stance_chain.invoke({\n",
    "    \"comment\": test_comment,\n",
    "    \"reply\": test_reply\n",
    "})\n",
    "\n",
    "print(f\" Final Classification: {result.strip()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with multiple examples to compare CoT vs regular classification\n",
    "print(\"\\n Comparison: CoT vs Regular Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"comment\": \"The new policy will improve workplace efficiency.\",\n",
    "        \"reply\": \"I agree completely, it addresses our main concerns.\",\n",
    "        \"expected\": \"agree\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Remote work is the future of employment.\",\n",
    "        \"reply\": \"I disagree, in-person collaboration is still essential.\",\n",
    "        \"expected\": \"disagree\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"The weather has been quite unpredictable this month.\",\n",
    "        \"reply\": \"That's an interesting observation about climate patterns.\",\n",
    "        \"expected\": \"neutral\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n Test Case {i}:\")\n",
    "    print(f\"Comment: {case['comment']}\")\n",
    "    print(f\"Reply: {case['reply']}\")\n",
    "    \n",
    "    # CoT classification\n",
    "    cot_result = cot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    # Regular classification for comparison\n",
    "    regular_result = stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    print(f\" CoT Result: {cot_result.strip()}\")\n",
    "    print(f\" Regular Result: {regular_result.strip()}\")\n",
    "    print(f\" Expected: {case['expected']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n Chain-of-Thought testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "###  How CoT Works\n",
    "\n",
    "Chain-of-Thought prompting is a technique that breaks down complex reasoning tasks into explicit intermediate steps. Instead of asking the model to directly classify the stance, we:\n",
    "\n",
    "1. **Stage 1 (Reasoning)**: Ask the model to explain WHY a reply has a particular stance toward a comment\n",
    "2. **Stage 2 (Classification)**: Use that reasoning to make the final classification\n",
    "\n",
    "###  Benefits of CoT Prompting\n",
    "\n",
    "- **Improved Accuracy**: By forcing explicit reasoning, models often make better decisions\n",
    "- **Transparency**: We can see the model's reasoning process\n",
    "- **Consistency**: The two-stage process reduces random classification errors\n",
    "- **Debugging**: If classification is wrong, we can examine the reasoning step\n",
    "\n",
    "###  Technical Implementation\n",
    "\n",
    "Our CoT chain uses `RunnablePassthrough` to:\n",
    "- Pass the original `comment` and `reply` through both stages\n",
    "- Generate `stance_reason` in stage 1\n",
    "- Use all three variables (`comment`, `reply`, `stance_reason`) in stage 2\n",
    "\n",
    "###  Expected Improvements\n",
    "\n",
    "CoT prompting typically shows improvements in:\n",
    "- **Complex cases** where the stance isn't immediately obvious\n",
    "- **Ambiguous replies** that could be interpreted multiple ways\n",
    "- **Consistency** across similar examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Examining the Chain-of-Thought Reasoning Process\n",
      "======================================================================\n",
      " Test Input:\n",
      "Comment: The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\n",
      "Reply: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
      "\n",
      " Stage 1: Generating Reasoning...\n",
      "--------------------------------------------------\n",
      " Model's Reasoning:\n",
      "The reply agrees with the comment by stating that while the movie may not perfectly capture the content, it still manages to convey the philosophy of Frank Herbert's work. The phrase \"if not the content\" suggests that the movie captures the spirit of the original material.\n",
      "\n",
      " Stage 2: Final Classification...\n",
      "--------------------------------------------------\n",
      " Final Classification:\n",
      "<|assistant|>\n",
      "agree\n",
      "\n",
      " This demonstrates how CoT provides:\n",
      "   Transparent reasoning process\n",
      "   Explicit justification for decisions\n",
      "   Ability to debug classification errors\n",
      "   More consistent and reliable results\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the reasoning step explicitly\n",
    "print(\" Examining the Chain-of-Thought Reasoning Process\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Let's manually run each stage to see the reasoning\n",
    "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
    "test_reply = \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\"\n",
    "\n",
    "print(\" Test Input:\")\n",
    "print(f\"Comment: {test_comment}\")\n",
    "print(f\"Reply: {test_reply}\")\n",
    "print()\n",
    "\n",
    "# Stage 1: Generate reasoning\n",
    "print(\" Stage 1: Generating Reasoning...\")\n",
    "print(\"-\" * 50)\n",
    "reasoning_result = reasoning_prompt.format(comment=test_comment, reply=test_reply)\n",
    "stance_reason = llm.invoke(reasoning_result)\n",
    "\n",
    "print(\" Model's Reasoning:\")\n",
    "print(stance_reason.strip())\n",
    "print()\n",
    "\n",
    "# Stage 2: Use reasoning for classification\n",
    "print(\" Stage 2: Final Classification...\")\n",
    "print(\"-\" * 50)\n",
    "classification_input = classification_prompt.format(\n",
    "    comment=test_comment, \n",
    "    reply=test_reply, \n",
    "    stance_reason=stance_reason.strip()\n",
    ")\n",
    "final_classification = llm.invoke(classification_input)\n",
    "\n",
    "print(\" Final Classification:\")\n",
    "print(final_classification.strip())\n",
    "print()\n",
    "\n",
    "print(\" This demonstrates how CoT provides:\")\n",
    "print(\"   Transparent reasoning process\")\n",
    "print(\"   Explicit justification for decisions\")\n",
    "print(\"   Ability to debug classification errors\")\n",
    "print(\"   More consistent and reliable results\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analyzing Test Case 2 Classification Issue\n",
      "======================================================================\n",
      " Test Case 2 Details:\n",
      "Comment: Remote work is the future of employment.\n",
      "Reply: I disagree, in-person collaboration is still essential.\n",
      "\n",
      " Running Chain-of-Thought Analysis...\n",
      "--------------------------------------------------\n",
      " Stage 1 - Model's Reasoning:\n",
      "<|assistant|>\n",
      "The reply is classified as \"disagree\" because the statement directly contradicts the comment, which asserts that remote work is the future of employment. The word \"disagree\" explicitly indicates a negative stance, suggesting that the speaker believes in-person collaboration remains crucial. The use of the word \"essential\" further emphasizes this disagreement by highlighting the importance of in-person collaboration, which is seen as a necessary component of employment in this context.\n",
      "\n",
      " Stage 2 - Final Classification:\n",
      "Result: <|assistant|>\n",
      "disagree\n",
      "\n",
      " Comparison:\n",
      "CoT Result: <|assistant|>\n",
      "disagree\n",
      "Regular Result: disagree\n",
      "\n",
      " Analysis:\n",
      " The reply explicitly states 'I disagree'\n",
      " This is a clear linguistic signal for disagreement\n",
      " Both methods should classify this as 'disagree'\n",
      " If you saw 'neutral', it might be due to:\n",
      "  - Model variability (different runs can give different results)\n",
      "  - Previous test with different examples\n",
      "  - Looking at a different test case\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#  Detailed Analysis of Test Case 2 - \"I disagree\" Classification\n",
    "print(\" Analyzing Test Case 2 Classification Issue\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Let's examine Test Case 2 specifically\n",
    "case2_comment = \"Remote work is the future of employment.\"\n",
    "case2_reply = \"I disagree, in-person collaboration is still essential.\"\n",
    "\n",
    "print(\" Test Case 2 Details:\")\n",
    "print(f\"Comment: {case2_comment}\")\n",
    "print(f\"Reply: {case2_reply}\")\n",
    "print()\n",
    "\n",
    "print(\" Running Chain-of-Thought Analysis...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Stage 1: Get the reasoning\n",
    "reasoning_input = reasoning_prompt.format(comment=case2_comment, reply=case2_reply)\n",
    "reasoning_output = llm.invoke(reasoning_input)\n",
    "\n",
    "print(\" Stage 1 - Model's Reasoning:\")\n",
    "print(reasoning_output.strip())\n",
    "print()\n",
    "\n",
    "# Stage 2: Get final classification\n",
    "classification_input = classification_prompt.format(\n",
    "    comment=case2_comment, \n",
    "    reply=case2_reply, \n",
    "    stance_reason=reasoning_output.strip()\n",
    ")\n",
    "final_classification = llm.invoke(classification_input)\n",
    "\n",
    "print(\" Stage 2 - Final Classification:\")\n",
    "print(f\"Result: {final_classification.strip()}\")\n",
    "print()\n",
    "\n",
    "# Also test the regular chain for comparison\n",
    "regular_result = stance_chain.invoke({\n",
    "    \"comment\": case2_comment,\n",
    "    \"reply\": case2_reply\n",
    "})\n",
    "\n",
    "print(\" Comparison:\")\n",
    "print(f\"CoT Result: {final_classification.strip()}\")\n",
    "print(f\"Regular Result: {regular_result.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\" Analysis:\")\n",
    "print(\" The reply explicitly states 'I disagree'\")\n",
    "print(\" This is a clear linguistic signal for disagreement\")\n",
    "print(\" Both methods should classify this as 'disagree'\")\n",
    "print(\" If you saw 'neutral', it might be due to:\")\n",
    "print(\"  - Model variability (different runs can give different results)\")\n",
    "print(\"  - Previous test with different examples\")\n",
    "print(\"  - Looking at a different test case\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-of-Thought Prompt\n",
    "\n",
    "Please generate code that uses Tree-of-Thought (ToT) prompting to explore multiple reasoning paths and iteratively evaluate potential stances (agree, disagree, neutral) before making a final classification. Maintain the context of the comment and reply through all stages of reasoning and evaluation.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "**Step 1 - Generate Hypotheses:**\n",
    "- Propose multiple possible stances (agree, disagree, neutral) based on different interpretations of the reply.\n",
    "- Use a prompt template to generate each hypothesis with explanations.\n",
    "- Output: hypotheses as a list.\n",
    "\n",
    "**Step 2 - Evaluate Hypotheses:**\n",
    "- Assess the validity of each hypothesis by critically analyzing its reasoning.\n",
    "- Use an evaluation prompt template to rank or score each hypothesis based on coherence and relevance. Assign a score (15) for logical consistency and coherence\n",
    "- Output: evaluations as scores or rankings for each hypothesis.\n",
    "\n",
    "**Step 3 - Final Decision:**\n",
    "- Select the hypothesis with the highest score or best reasoning and output the final stance as \"agree\", \"disagree\", or \"neutral\" based on the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Implementing Tree-of-Thought (ToT) Stance Classification\n",
      "======================================================================\n",
      " Step 1 - Hypothesis Generation Template Created\n",
      " Step 2 - Hypothesis Evaluation Template Created\n",
      " Step 3 - Final Decision Template Created\n",
      " Tree-of-Thought Structure: Input  Hypotheses  Evaluation  Decision\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tree-of-Thought Prompt Implementation\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\" Implementing Tree-of-Thought (ToT) Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Generate Hypotheses - Explore multiple reasoning paths\n",
    "hypothesis_template = \"\"\"Given the following comment and reply, generate three different hypotheses about the stance of the reply toward the comment. For each hypothesis, provide the stance (agree, disagree, or neutral) and a detailed explanation of the reasoning.\n",
    "\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "\n",
    "Generate exactly 3 hypotheses in the following format:\n",
    "\n",
    "Hypothesis 1: [stance]\n",
    "Reasoning: [detailed explanation]\n",
    "\n",
    "Hypothesis 2: [stance] \n",
    "Reasoning: [detailed explanation]\n",
    "\n",
    "Hypothesis 3: [stance]\n",
    "Reasoning: [detailed explanation]\n",
    "\n",
    "Focus on different possible interpretations of the reply's meaning and intent.\"\"\"\n",
    "\n",
    "hypothesis_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    template=hypothesis_template\n",
    ")\n",
    "\n",
    "print(\" Step 1 - Hypothesis Generation Template Created\")\n",
    "\n",
    "# Step 2: Evaluate Hypotheses - Score each hypothesis for logical consistency\n",
    "evaluation_template = \"\"\"Evaluate the following hypotheses about the stance classification. For each hypothesis, assign a score from 1-5 based on logical consistency and coherence with the given comment and reply.\n",
    "\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "\n",
    "Hypotheses to evaluate:\n",
    "{hypotheses}\n",
    "\n",
    "For each hypothesis, provide:\n",
    "- Score (1-5, where 5 is most logical and coherent)\n",
    "- Brief justification\n",
    "\n",
    "Format your response as:\n",
    "Hypothesis 1 Score: [1-5]\n",
    "Justification: [brief explanation]\n",
    "\n",
    "Hypothesis 2 Score: [1-5] \n",
    "Justification: [brief explanation]\n",
    "\n",
    "Hypothesis 3 Score: [1-5]\n",
    "Justification: [brief explanation]\"\"\"\n",
    "\n",
    "evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"hypotheses\"],\n",
    "    template=evaluation_template\n",
    ")\n",
    "\n",
    "print(\" Step 2 - Hypothesis Evaluation Template Created\")\n",
    "\n",
    "# Step 3: Final Decision - Select best hypothesis and output final stance\n",
    "decision_template = \"\"\"Based on the evaluation scores and justifications, select the best hypothesis and provide the final stance classification.\n",
    "\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "\n",
    "Hypotheses and Evaluations:\n",
    "{evaluations}\n",
    "\n",
    "Select the hypothesis with the highest score and provide the final stance as a single word: \"agree\", \"disagree\", or \"neutral\".\n",
    "\n",
    "Final Decision: \"\"\"\n",
    "\n",
    "decision_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"evaluations\"],\n",
    "    template=decision_template\n",
    ")\n",
    "\n",
    "print(\" Step 3 - Final Decision Template Created\")\n",
    "print(\" Tree-of-Thought Structure: Input  Hypotheses  Evaluation  Decision\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building Tree-of-Thought Processing Chain...\n",
      "--------------------------------------------------\n",
      " Tree-of-Thought Chain Created Successfully!\n",
      " Processing Flow:\n",
      "   1. Generate 3 different hypotheses with reasoning\n",
      "   2. Evaluate each hypothesis (score 1-5)\n",
      "   3. Select best hypothesis for final classification\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the Tree-of-Thought Chain\n",
    "print(\" Building Tree-of-Thought Processing Chain...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Helper function for evaluation step\n",
    "def create_evaluation_input(x):\n",
    "    return evaluation_prompt.format(\n",
    "        comment=x[\"comment\"],\n",
    "        reply=x[\"reply\"], \n",
    "        hypotheses=x[\"hypotheses\"]\n",
    "    )\n",
    "\n",
    "# Helper function for decision step  \n",
    "def create_decision_input(x):\n",
    "    return decision_prompt.format(\n",
    "        comment=x[\"comment\"],\n",
    "        reply=x[\"reply\"],\n",
    "        evaluations=x[\"evaluations\"]\n",
    "    )\n",
    "\n",
    "# Tree-of-Thought chain implementation using RunnablePassthrough\n",
    "tot_stance_chain = (\n",
    "    {\n",
    "        \"comment\": RunnablePassthrough(),\n",
    "        \"reply\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        # Step 1: Generate hypotheses\n",
    "        hypotheses=hypothesis_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        # Step 2: Evaluate hypotheses  \n",
    "        evaluations=create_evaluation_input | llm | StrOutputParser()\n",
    "    )\n",
    "    | create_decision_input\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\" Tree-of-Thought Chain Created Successfully!\")\n",
    "print(\" Processing Flow:\")\n",
    "print(\"   1. Generate 3 different hypotheses with reasoning\")\n",
    "print(\"   2. Evaluate each hypothesis (score 1-5)\")\n",
    "print(\"   3. Select best hypothesis for final classification\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tree-of-Thought Stance Classification\n",
    "print(\" Testing Tree-of-Thought Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with a complex, ambiguous case\n",
    "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
    "test_reply = \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\"\n",
    "\n",
    "print(\" Test Case (Ambiguous - good for ToT analysis):\")\n",
    "print(f\"Comment: {test_comment}\")\n",
    "print(f\"Reply: {test_reply}\")\n",
    "print()\n",
    "\n",
    "print(\" Tree-of-Thought Processing...\")\n",
    "print(\"This will generate multiple hypotheses, evaluate them, and select the best one.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Run the ToT chain\n",
    "tot_result = tot_stance_chain.invoke({\n",
    "    \"comment\": test_comment,\n",
    "    \"reply\": test_reply\n",
    "})\n",
    "\n",
    "print(f\" Final ToT Classification: {tot_result.strip()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compare all three methods: Regular, CoT, and ToT\n",
    "print(\"\\n Method Comparison: Regular vs CoT vs ToT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_cases = [\n",
    "    {\n",
    "        \"comment\": \"Remote work is destroying team collaboration and productivity.\",\n",
    "        \"reply\": \"I partially agree - while productivity can suffer, remote work also offers flexibility benefits.\",\n",
    "        \"description\": \"Complex case with partial agreement\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"The new AI regulation will stifle innovation in the tech industry.\",\n",
    "        \"reply\": \"That's a valid concern, though some oversight might be necessary for safety.\",\n",
    "        \"description\": \"Nuanced response with acknowledgment but neutral stance\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Climate change is the most urgent global challenge we face today.\",\n",
    "        \"reply\": \"Absolutely, we need immediate action on carbon emissions and renewable energy.\",\n",
    "        \"description\": \"Clear agreement case\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(comparison_cases, 1):\n",
    "    print(f\"\\n Comparison Case {i}: {case['description']}\")\n",
    "    print(f\"Comment: {case['comment']}\")\n",
    "    print(f\"Reply: {case['reply']}\")\n",
    "    \n",
    "    # Regular classification\n",
    "    regular_result = stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    # CoT classification\n",
    "    cot_result = cot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    # ToT classification\n",
    "    tot_result = tot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    print(f\" Regular: {regular_result.strip()}\")\n",
    "    print(f\" CoT: {cot_result.strip()}\")\n",
    "    print(f\" ToT: {tot_result.strip()}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n Tree-of-Thought testing completed!\")\n",
    "print(\" ToT provides the most thorough analysis by:\")\n",
    "print(\"    Exploring multiple interpretation paths\")\n",
    "print(\"    Scoring hypotheses for logical consistency\") \n",
    "print(\"    Selecting the most coherent reasoning\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step-by-Step Tree-of-Thought Process Demonstration\n",
      "======================================================================\n",
      " Demonstration Case:\n",
      "Comment: The new government policy on renewable energy is too aggressive and will hurt the economy.\n",
      "Reply: While the timeline is ambitious, investing in clean energy could create new economic opportunities.\n",
      "\n",
      " STEP 1: Generate Multiple Hypotheses\n",
      "--------------------------------------------------\n",
      " Generated Hypotheses:\n",
      "<|assistant|>\n",
      "Hypothesis 1: Neutral\n",
      "Reasoning: The reply acknowledges the criticism about the aggressive timeline but shifts the focus to potential economic benefits of the policy. This suggests a balanced view rather than a clear stance.\n",
      "\n",
      "Hypothesis 2: Agree\n",
      "Reasoning: The reply supports the idea that investing in renewable energy could create new economic opportunities, even if it challenges the timeline. This indicates a positive outlook on the economic potential of the policy.\n",
      "\n",
      "Hypothesis 3: Disagree \n",
      "Reasoning: The reply highlights the negative impact of the policy on the economy, contradicting the comment's assertion. This shows a disagreement with the comment's viewpoint.\n",
      "\n",
      " STEP 2: Evaluate Each Hypothesis\n",
      "--------------------------------------------------\n",
      " Hypothesis Evaluations:\n",
      "\n",
      "\n",
      " STEP 3: Final Decision Based on Evaluations\n",
      "--------------------------------------------------\n",
      " Final Classification:\n",
      "<|assistant|>\n",
      "neutral\n",
      "\n",
      " Tree-of-Thought Process Benefits:\n",
      "   Multiple perspectives considered simultaneously\n",
      "   Explicit evaluation of reasoning quality\n",
      "   Selection based on logical consistency\n",
      "   Reduced bias from single interpretation path\n",
      "   More robust classification for ambiguous cases\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed Tree-of-Thought Process Demonstration\n",
    "print(\" Step-by-Step Tree-of-Thought Process Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use a challenging example for demonstration\n",
    "demo_comment = \"The new government policy on renewable energy is too aggressive and will hurt the economy.\"\n",
    "demo_reply = \"While the timeline is ambitious, investing in clean energy could create new economic opportunities.\"\n",
    "\n",
    "print(\" Demonstration Case:\")\n",
    "print(f\"Comment: {demo_comment}\")\n",
    "print(f\"Reply: {demo_reply}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Generate Hypotheses\n",
    "print(\" STEP 1: Generate Multiple Hypotheses\")\n",
    "print(\"-\" * 50)\n",
    "hypotheses_input = hypothesis_prompt.format(comment=demo_comment, reply=demo_reply)\n",
    "hypotheses_output = llm.invoke(hypotheses_input)\n",
    "\n",
    "print(\" Generated Hypotheses:\")\n",
    "print(hypotheses_output.strip())\n",
    "print()\n",
    "\n",
    "# Step 2: Evaluate Hypotheses\n",
    "print(\" STEP 2: Evaluate Each Hypothesis\")\n",
    "print(\"-\" * 50)\n",
    "evaluation_input = evaluation_prompt.format(\n",
    "    comment=demo_comment,\n",
    "    reply=demo_reply,\n",
    "    hypotheses=hypotheses_output.strip()\n",
    ")\n",
    "evaluation_output = llm.invoke(evaluation_input)\n",
    "\n",
    "print(\" Hypothesis Evaluations:\")\n",
    "print(evaluation_output.strip())\n",
    "print()\n",
    "\n",
    "# Step 3: Final Decision\n",
    "print(\" STEP 3: Final Decision Based on Evaluations\")\n",
    "print(\"-\" * 50)\n",
    "decision_input = decision_prompt.format(\n",
    "    comment=demo_comment,\n",
    "    reply=demo_reply,\n",
    "    evaluations=evaluation_output.strip()\n",
    ")\n",
    "final_decision = llm.invoke(decision_input)\n",
    "\n",
    "print(\" Final Classification:\")\n",
    "print(final_decision.strip())\n",
    "print()\n",
    "\n",
    "print(\" Tree-of-Thought Process Benefits:\")\n",
    "print(\"   Multiple perspectives considered simultaneously\")\n",
    "print(\"   Explicit evaluation of reasoning quality\")\n",
    "print(\"   Selection based on logical consistency\")\n",
    "print(\"   Reduced bias from single interpretation path\")\n",
    "print(\"   More robust classification for ambiguous cases\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tree-of-Thought (ToT) Prompting\n",
    "\n",
    "###  Tree-of-Thought Methodology\n",
    "\n",
    "Tree-of-Thought prompting represents an advanced reasoning technique that explores multiple solution paths simultaneously, evaluates them systematically, and selects the most coherent approach. Unlike linear reasoning methods, ToT creates a \"tree\" of possible interpretations.\n",
    "\n",
    "###  Three-Stage Process\n",
    "\n",
    "1. **Hypothesis Generation**: Create multiple possible interpretations of the same input\n",
    "   - Generate 3 different stance hypotheses (agree, disagree, neutral)\n",
    "   - Each hypothesis includes detailed reasoning\n",
    "   - Explores different aspects and interpretations\n",
    "\n",
    "2. **Systematic Evaluation**: Score each hypothesis objectively\n",
    "   - Assign numerical scores (1-5) for logical consistency\n",
    "   - Provide justifications for each score\n",
    "   - Compare reasoning quality across hypotheses\n",
    "\n",
    "3. **Best Path Selection**: Choose the highest-scoring interpretation\n",
    "   - Select hypothesis with strongest logical foundation\n",
    "   - Output final classification based on best reasoning\n",
    "   - Reduces single-path bias and errors\n",
    "\n",
    "###  Advantages Over Other Methods\n",
    "\n",
    "| Method | Reasoning Paths | Evaluation | Best For |\n",
    "|--------|----------------|------------|----------|\n",
    "| **Regular** | Single, direct | None | Clear, obvious cases |\n",
    "| **Chain-of-Thought** | Single, detailed | Implicit | Transparent reasoning |\n",
    "| **Tree-of-Thought** | Multiple, evaluated | Explicit scoring | Complex, ambiguous cases |\n",
    "\n",
    "###  When to Use ToT\n",
    "\n",
    "- **Ambiguous inputs** where multiple interpretations are valid\n",
    "- **High-stakes decisions** requiring thorough analysis\n",
    "- **Complex reasoning** tasks with subtle nuances\n",
    "- **Quality assurance** for critical classifications\n",
    "\n",
    "###  Expected Outcomes\n",
    "\n",
    "ToT typically provides:\n",
    "- **Higher accuracy** on difficult cases\n",
    "- **Better handling** of ambiguous language\n",
    "- **More consistent** results across similar examples\n",
    "- **Traceable reasoning** with explicit evaluation criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Consistency Prompt\n",
    "\n",
    "Given the final stance labels from multiple reasoning approachesTree-of-Thought (ToT), Chain-of-Thought (CoT), Few-Shot prompting, and Task-only approachdetermine the final stance label (\"agree\", \"disagree\", or \"neutral\") by synthesizing the outputs.\n",
    "\n",
    "## Inputs:\n",
    "- **Comment and Reply**: The context for determining the stance.\n",
    "- **Stance Labels from the four approaches**:\n",
    "    - `tot_output`: Stance label from the Tree-of-Thought (ToT) approach.\n",
    "    - `cot_output`: Stance label from the Chain-of-Thought (CoT) approach.\n",
    "    - `few_shot_output`: Stance label from the Few-Shot prompting approach.\n",
    "    - `task_output`: Stance label from the Task-only approach.\n",
    "\n",
    "## Steps:\n",
    "1. Compare the stance labels from the four approaches.\n",
    "2. Identify patterns of agreement or disagreement:\n",
    "   - If there is a majority consensus, select that stance label.\n",
    "   - If there is no clear majority, resolve the inconsistencies by choosing the most consistent or compelling label based on the distribution of outputs.\n",
    "3. **Output**: A final stance label of \"agree\", \"disagree\", or \"neutral\", based on the most consistent or majority label. Only output the label, with no additional explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Implementing Self-Consistency Stance Classification\n",
      "======================================================================\n",
      " Setting up Few-Shot Prompting Chain...\n",
      " Few-Shot Chain Created\n",
      " Self-Consistency Template Created\n",
      " Integration: Task-only  CoT  Few-Shot  ToT  Self-Consistency\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Self-Consistency Implementation\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\" Implementing Self-Consistency Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First, let's set up the Few-Shot chain that we need for self-consistency\n",
    "print(\" Setting up Few-Shot Prompting Chain...\")\n",
    "\n",
    "# Define the template for each individual example\n",
    "example_template = '''comment: {comment}\n",
    "reply: {reply}\n",
    "stance: {stance}'''\n",
    "\n",
    "# Create the example prompt template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Create diverse few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        'comment': \"I think the new policy will help improve efficiency.\",\n",
    "        'reply': \"I agree, it will definitely streamline our operations.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The new education reform seems promising.\", \n",
    "        'reply': \"I disagree, it doesn't address the underlying issues.\",\n",
    "        'stance': 'disagree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The park renovation project is a good idea.\",\n",
    "        'reply': \"I'm not sure. The location might be an issue.\",\n",
    "        'stance': 'neutral'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"Artificial intelligence will revolutionize healthcare.\",\n",
    "        'reply': \"Absolutely, it has the potential to save many lives.\",\n",
    "        'stance': 'agree'\n",
    "    },\n",
    "    {\n",
    "        'comment': \"The economy is showing signs of recovery after the pandemic.\",\n",
    "        'reply': \"I disagree, the recovery seems slow and uneven across sectors.\",\n",
    "        'stance': 'disagree'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the prefix and suffix for few-shot prompting\n",
    "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
    "\n",
    "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
    "comment: {comment}\n",
    "reply: {reply}\n",
    "stance:'''\n",
    "\n",
    "# Create the FewShotPromptTemplate\n",
    "few_shot_stance_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"comment\", \"reply\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create few-shot chain\n",
    "few_shot_stance_chain = few_shot_stance_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\" Few-Shot Chain Created\")\n",
    "\n",
    "# Now implement the Self-Consistency template\n",
    "consistency_template = '''Consider the following comment and reply:\n",
    "Comment: {comment}\n",
    "Reply: {reply}\n",
    "\n",
    "You have been provided with stance outputs generated by four different approaches:\n",
    "1. Tree-of-Thought (ToT) approach: {tot_output}\n",
    "2. Chain-of-Thought (CoT) approach: {cot_output}\n",
    "3. Few-Shot approach: {few_shot_output}\n",
    "4. Task-only approach: {task_output}\n",
    "\n",
    "Compare these outputs and determine the most likely stance label. If there is a majority consensus among the approaches, select that stance. If there is no clear majority, choose the most consistent or compelling label based on the distribution of outputs.\n",
    "\n",
    "Output the final stance label as \"agree\", \"disagree\", or \"neutral\" based on the most consistency across the responses. Only output the stance label as a single word and do not generate any other text after the label.\n",
    "\n",
    "Final stance:'''\n",
    "\n",
    "# Create the Self-Consistency prompt template\n",
    "consistency_prompt = PromptTemplate(\n",
    "    input_variables=[\"comment\", \"reply\", \"tot_output\", \"cot_output\", \"few_shot_output\", \"task_output\"],\n",
    "    template=consistency_template\n",
    ")\n",
    "\n",
    "print(\" Self-Consistency Template Created\")\n",
    "print(\" Integration: Task-only  CoT  Few-Shot  ToT  Self-Consistency\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building Self-Consistency Processing Chain...\n",
      "--------------------------------------------------\n",
      " Self-Consistency Chain Created Successfully!\n",
      " Processing Flow:\n",
      "   1. Run Task-only approach (Regular prompt)\n",
      "   2. Run Chain-of-Thought approach\n",
      "   3. Run Few-Shot approach\n",
      "   4. Run Tree-of-Thought approach\n",
      "   5. Synthesize all outputs for final decision\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the Self-Consistency Chain\n",
    "print(\" Building Self-Consistency Processing Chain...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Self-Consistency chain that runs all four approaches and synthesizes results\n",
    "self_consistency_chain = (\n",
    "    {\n",
    "        \"comment\": RunnablePassthrough(),\n",
    "        \"reply\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        # Run all four approaches in parallel\n",
    "        task_output=stance_chain,\n",
    "        cot_output=cot_stance_chain,\n",
    "        few_shot_output=few_shot_stance_chain,\n",
    "        tot_output=tot_stance_chain\n",
    "    )\n",
    "    | consistency_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\" Self-Consistency Chain Created Successfully!\")\n",
    "print(\" Processing Flow:\")\n",
    "print(\"   1. Run Task-only approach (Regular prompt)\")\n",
    "print(\"   2. Run Chain-of-Thought approach\")  \n",
    "print(\"   3. Run Few-Shot approach\")\n",
    "print(\"   4. Run Tree-of-Thought approach\")\n",
    "print(\"   5. Synthesize all outputs for final decision\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing Self-Consistency Stance Classification\n",
      "======================================================================\n",
      "\n",
      " Self-Consistency Test Case 1: Nuanced response - acknowledges concern but suggests need for regulation\n",
      "Comment: The new AI regulation will stifle innovation in the tech industry.\n",
      "Reply: That's a valid concern, though some oversight might be necessary for safety.\n",
      "\n",
      " Running All Four Approaches...\n",
      " Task-only: neutral\n",
      " Chain-of-Thought: <|assistant|>\n",
      "disagree\n",
      " Few-Shot: neutral\n",
      "<|assistant|>\n",
      "neutral\n",
      " Tree-of-Thought: \"agree\"\n",
      " Self-Consistency Final: <|assistant|>\n",
      "agree\n",
      "--------------------------------------------------\n",
      "\n",
      " Self-Consistency Test Case 2: Complex case with partial agreement and counterpoint\n",
      "Comment: Remote work is destroying team collaboration and productivity.\n",
      "Reply: I partially agree - while productivity can suffer, remote work also offers flexibility benefits.\n",
      "\n",
      " Running All Four Approaches...\n",
      " Task-only: neutral\n",
      " Chain-of-Thought: <|assistant|>\n",
      "neutral\n",
      " Few-Shot: neutral\n",
      "<|assistant|>\n",
      "neutral\n",
      " Tree-of-Thought: <|assistant|>\n",
      "disagree\n",
      " Self-Consistency Final: <|assistant|>\n",
      "partial agree\n",
      "--------------------------------------------------\n",
      "\n",
      " Self-Consistency Test Case 3: Ambiguous case - seems to disagree but offers alternative perspective\n",
      "Comment: The new Dune movie does not really capture the vision laid out by Frank Herbert.\n",
      "Reply: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
      "\n",
      " Running All Four Approaches...\n",
      " Task-only: agree\n",
      " Chain-of-Thought: agree\n",
      " Few-Shot: disagree\n",
      "<|assistant|>\n",
      "disagree\n",
      " Tree-of-Thought: {'stature': 'disagree','reason': 'The highest scoring hypothesis is 5, indicating strong disagreement with the original comment.'}\n",
      " Self-Consistency Final: disagree\n",
      "--------------------------------------------------\n",
      "\n",
      " Self-Consistency testing completed!\n",
      " Self-Consistency provides the most robust classification by:\n",
      "    Running multiple reasoning approaches\n",
      "    Identifying consensus across methods\n",
      "    Resolving inconsistencies intelligently\n",
      "    Reducing single-method bias\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Self-Consistency Stance Classification\n",
    "print(\" Testing Self-Consistency Stance Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with challenging cases that might produce different results across methods\n",
    "test_cases = [\n",
    "    {\n",
    "        \"comment\": \"The new AI regulation will stifle innovation in the tech industry.\",\n",
    "        \"reply\": \"That's a valid concern, though some oversight might be necessary for safety.\",\n",
    "        \"description\": \"Nuanced response - acknowledges concern but suggests need for regulation\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Remote work is destroying team collaboration and productivity.\",\n",
    "        \"reply\": \"I partially agree - while productivity can suffer, remote work also offers flexibility benefits.\",\n",
    "        \"description\": \"Complex case with partial agreement and counterpoint\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"The new Dune movie does not really capture the vision laid out by Frank Herbert.\",\n",
    "        \"reply\": \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
    "        \"description\": \"Ambiguous case - seems to disagree but offers alternative perspective\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n Self-Consistency Test Case {i}: {case['description']}\")\n",
    "    print(f\"Comment: {case['comment']}\")\n",
    "    print(f\"Reply: {case['reply']}\")\n",
    "    print()\n",
    "    \n",
    "    # Run self-consistency analysis\n",
    "    print(\" Running All Four Approaches...\")\n",
    "    \n",
    "    # Get individual results first for comparison\n",
    "    task_result = stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    cot_result = cot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    few_shot_result = few_shot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    tot_result = tot_stance_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    print(f\" Task-only: {task_result.strip()}\")\n",
    "    print(f\" Chain-of-Thought: {cot_result.strip()}\")\n",
    "    print(f\" Few-Shot: {few_shot_result.strip()}\")\n",
    "    print(f\" Tree-of-Thought: {tot_result.strip()}\")\n",
    "    \n",
    "    # Run self-consistency\n",
    "    consistency_result = self_consistency_chain.invoke({\n",
    "        \"comment\": case[\"comment\"],\n",
    "        \"reply\": case[\"reply\"]\n",
    "    })\n",
    "    \n",
    "    print(f\" Self-Consistency Final: {consistency_result.strip()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n Self-Consistency testing completed!\")\n",
    "print(\" Self-Consistency provides the most robust classification by:\")\n",
    "print(\"    Running multiple reasoning approaches\")\n",
    "print(\"    Identifying consensus across methods\")\n",
    "print(\"    Resolving inconsistencies intelligently\")\n",
    "print(\"    Reducing single-method bias\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step-by-Step Self-Consistency Process Demonstration\n",
      "======================================================================\n",
      " Demonstration Case:\n",
      "Comment: Climate change is the most urgent global challenge we face today.\n",
      "Reply: While it's certainly important, economic recovery should be our immediate priority right now.\n",
      "\n",
      " STEP 1-4: Running Individual Approaches\n",
      "--------------------------------------------------\n",
      " Task-only Approach:\n",
      "Result: disagree\n",
      "\n",
      " Chain-of-Thought Approach:\n",
      "Result: <|assistant|>\n",
      "disagree\n",
      "\n",
      " Few-Shot Approach:\n",
      "Result: disagree\n",
      "\n",
      "<|assistant|>\n",
      "disagree\n",
      "\n",
      " Tree-of-Thought Approach:\n",
      "Result: Based on the evaluation scores and justifications, the best hypothesis is Hypothesis 2 with a score of 4. This hypothesis best captures the neutral stance of the reply, where neither the comment nor the reply strongly supports or opposes each other, but instead maintains a balanced view.\n",
      "\n",
      "Final Stance: \"neutral\"\n",
      "\n",
      " STEP 5: Self-Consistency Synthesis\n",
      "--------------------------------------------------\n",
      " Input to Self-Consistency Model:\n",
      "Consider the following comment and reply:\n",
      "Comment: Climate change is the most urgent global challenge we face today.\n",
      "Reply: While it's certainly important, economic recovery should be our immediate priority right now.\n",
      "\n",
      "You have been provided with stance outputs generated by four different approaches:\n",
      "1. Tree-of-Thought (ToT) approach: Based on the evaluation scores and justifications, the best hypothesis is Hypothesis 2 with a score of 4. This hypothesis best captures the neutral stance of the reply, where neither the comment nor the reply strongly supports or opposes each other, but instead maintains a balanced view.\n",
      "\n",
      "Final Stance: \"neutral\"\n",
      "2. Chain-of-Thought (CoT) approach: <|assistant|>\n",
      "disagree\n",
      "3. Few-Shot approach: disagree\n",
      "\n",
      "<|assistant|>\n",
      "disagree\n",
      "4. Task-only approach: disagree\n",
      "\n",
      "Compare these outputs and determine the most likely stance label. If there is a majority consensus among the approaches, select that stance. If there is no clear majority, choose the most consistent or compelling label based on the distribution of outputs.\n",
      "\n",
      "Output the final stance label as \"agree\", \"disagree\", or \"neutral\" based on the most consistency across the responses. Only output the stance label as a single word and do not generate any other text after the label.\n",
      "\n",
      "Final stance:\n",
      "\n",
      " Final Self-Consistency Decision:\n",
      "Result: <assistant|>\n",
      "disagree\n",
      "\n",
      " Analysis Summary:\n",
      "Individual Results: ['disagree', '<|assistant|>\\ndisagree', 'disagree\\n\\n<|assistant|>\\ndisagree', 'Based on the evaluation scores and justifications, the best hypothesis is Hypothesis 2 with a score of 4. This hypothesis best captures the neutral stance of the reply, where neither the comment nor the reply strongly supports or opposes each other, but instead maintains a balanced view.\\n\\nFinal Stance: \"neutral\"']\n",
      "Unique Stances: ['Based on the evaluation scores and justifications, the best hypothesis is Hypothesis 2 with a score of 4. This hypothesis best captures the neutral stance of the reply, where neither the comment nor the reply strongly supports or opposes each other, but instead maintains a balanced view.\\n\\nFinal Stance: \"neutral\"', 'disagree', 'disagree\\n\\n<|assistant|>\\ndisagree', '<|assistant|>\\ndisagree']\n",
      "Most Common: No majority\n",
      "Self-Consistency Choice: <assistant|>\n",
      "disagree\n",
      "\n",
      " Self-Consistency Process Benefits:\n",
      "   Ensemble approach reduces individual method errors\n",
      "   Intelligent synthesis handles disagreements\n",
      "   More robust than any single reasoning method\n",
      "   Provides confidence through consensus\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed Self-Consistency Process Demonstration\n",
    "print(\" Step-by-Step Self-Consistency Process Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use a complex example for demonstration\n",
    "demo_comment = \"Climate change is the most urgent global challenge we face today.\"\n",
    "demo_reply = \"While it's certainly important, economic recovery should be our immediate priority right now.\"\n",
    "\n",
    "print(\" Demonstration Case:\")\n",
    "print(f\"Comment: {demo_comment}\")\n",
    "print(f\"Reply: {demo_reply}\")\n",
    "print()\n",
    "\n",
    "print(\" STEP 1-4: Running Individual Approaches\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 1: Task-only (Regular)\n",
    "print(\" Task-only Approach:\")\n",
    "task_result = stance_chain.invoke({\n",
    "    \"comment\": demo_comment,\n",
    "    \"reply\": demo_reply\n",
    "})\n",
    "print(f\"Result: {task_result.strip()}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Chain-of-Thought\n",
    "print(\" Chain-of-Thought Approach:\")\n",
    "cot_result = cot_stance_chain.invoke({\n",
    "    \"comment\": demo_comment,\n",
    "    \"reply\": demo_reply\n",
    "})\n",
    "print(f\"Result: {cot_result.strip()}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Few-Shot\n",
    "print(\" Few-Shot Approach:\")\n",
    "few_shot_result = few_shot_stance_chain.invoke({\n",
    "    \"comment\": demo_comment,\n",
    "    \"reply\": demo_reply\n",
    "})\n",
    "print(f\"Result: {few_shot_result.strip()}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Tree-of-Thought\n",
    "print(\" Tree-of-Thought Approach:\")\n",
    "tot_result = tot_stance_chain.invoke({\n",
    "    \"comment\": demo_comment,\n",
    "    \"reply\": demo_reply\n",
    "})\n",
    "print(f\"Result: {tot_result.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\" STEP 5: Self-Consistency Synthesis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create the consistency input manually to show the process\n",
    "consistency_input = consistency_prompt.format(\n",
    "    comment=demo_comment,\n",
    "    reply=demo_reply,\n",
    "    task_output=task_result.strip(),\n",
    "    cot_output=cot_result.strip(),\n",
    "    few_shot_output=few_shot_result.strip(),\n",
    "    tot_output=tot_result.strip()\n",
    ")\n",
    "\n",
    "print(\" Input to Self-Consistency Model:\")\n",
    "print(consistency_input)\n",
    "print()\n",
    "\n",
    "# Get final result\n",
    "final_result = llm.invoke(consistency_input)\n",
    "print(\" Final Self-Consistency Decision:\")\n",
    "print(f\"Result: {final_result.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\" Analysis Summary:\")\n",
    "results = [task_result.strip(), cot_result.strip(), few_shot_result.strip(), tot_result.strip()]\n",
    "unique_results = list(set(results))\n",
    "print(f\"Individual Results: {results}\")\n",
    "print(f\"Unique Stances: {unique_results}\")\n",
    "print(f\"Most Common: {max(set(results), key=results.count) if len(set(results)) < len(results) else 'No majority'}\")\n",
    "print(f\"Self-Consistency Choice: {final_result.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\" Self-Consistency Process Benefits:\")\n",
    "print(\"   Ensemble approach reduces individual method errors\")\n",
    "print(\"   Intelligent synthesis handles disagreements\")\n",
    "print(\"   More robust than any single reasoning method\")\n",
    "print(\"   Provides confidence through consensus\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Self-Consistency Prompting\n",
    "\n",
    "###  Self-Consistency Methodology\n",
    "\n",
    "Self-Consistency prompting represents the pinnacle of robust reasoning by combining multiple independent approaches and synthesizing their outputs. Rather than relying on a single reasoning path, it leverages the \"wisdom of crowds\" principle applied to different AI reasoning strategies.\n",
    "\n",
    "###  Four-Approach Ensemble\n",
    "\n",
    "Our Self-Consistency implementation combines:\n",
    "\n",
    "1. **Task-only (Regular)**: Direct, straightforward classification\n",
    "2. **Chain-of-Thought (CoT)**: Linear reasoning with explicit steps\n",
    "3. **Few-Shot**: Learning from examples and patterns\n",
    "4. **Tree-of-Thought (ToT)**: Multiple hypotheses with evaluation\n",
    "\n",
    "###  Synthesis Process\n",
    "\n",
    "| Step | Process | Purpose |\n",
    "|------|---------|---------|\n",
    "| **Parallel Execution** | Run all four approaches simultaneously | Generate diverse perspectives |\n",
    "| **Consensus Detection** | Identify majority agreement | Find confident classifications |\n",
    "| **Conflict Resolution** | Analyze disagreements intelligently | Handle ambiguous cases |\n",
    "| **Final Decision** | Synthesize into single output | Provide most robust answer |\n",
    "\n",
    "###  Consensus Rules\n",
    "\n",
    "- **Majority Consensus**: If 3+ methods agree  Select majority stance\n",
    "- **Split Decision**: If 2-2 split  Analyze reasoning quality and context\n",
    "- **No Consensus**: If all different  Weight by method reliability and reasoning strength\n",
    "\n",
    "###  When Self-Consistency Excels\n",
    "\n",
    "- **High-stakes decisions** requiring maximum confidence\n",
    "- **Ambiguous inputs** where single methods might fail\n",
    "- **Quality assurance** for critical classification tasks\n",
    "- **Research applications** where accuracy is paramount\n",
    "\n",
    "###  Expected Benefits\n",
    "\n",
    "Self-Consistency typically provides:\n",
    "- **Highest accuracy** through ensemble averaging\n",
    "- **Reduced bias** from any single reasoning approach\n",
    "- **Increased confidence** through consensus measurement\n",
    "- **Error detection** when methods disagree significantly\n",
    "\n",
    "###  Complete Reasoning Hierarchy\n",
    "\n",
    "```\n",
    "Simple  Complex  Robust\n",
    "Task-only  CoT  Few-Shot  ToT  Self-Consistency\n",
    "```\n",
    "\n",
    "Each level builds upon the previous, culminating in Self-Consistency as the most comprehensive approach for stance classification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agentic_ai_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
